---
layout: post
title: "xr vr mini project list"
name: "xr-motion"
tags: [xr]
tagName: xr
permalink: 2022-07-09-xr-motion.html
sidebar: other_sidebar
folder: blog
collection: wiki
categories: xr-school
keywords: "xr ar kinect facemask tracking landmark sports k-sports"
summary: "Sat, Jul 09, 22, facemask and sports kinect using object tracking and facial landmark"
excerpt_separator: <!--more-->
toc: true
public: true
parent: [[Wiki-Setting-Category]] 
date: 2022-07-09T14:41:00 +0900
updated: 2022-07-09 14:41
youtubeId: V2nimk44gQk
youtubeId2: V3tPRI7vRZ0
youtubeId3: onutM3HzVoo
youtubeId4: uL_XsdqQ1BE
---
* TOC
{:toc}

{{site.data.alerts.callout_warning}}This is a draft, the content is not complete and of poor quality!{{site.data.alerts.end}}

## Tech list
- [ ] to do list for future mini projects
- 

- [ ] my museum
- [ ] face tracking
- [ ] aquarium ar ğŸ“¦ arkit
- [ ] geo location based app
- [ ] pokemon go clone ğŸ‘ğŸ½ vuforia mapbox geo-location backend
- [ ] dynamic image/video 3d model upload system ğŸ¦˜ vuforia mapbox geo-location
- [ ] restaurant menu app ğŸ§· arkit 3d app-design backend ğŸ‘¶ğŸ½ arkit 3d 
- [ ] live animal 4d ğŸ¥‡ vuforia, mapbox, geolocation backend-api
- [ ] pro boxing with politicians ğŸ‘ğŸ½ unity arkit
- [ ]  ar color stack ğŸ‘ğŸ½  arkit
- [ ] 3d dynamics books ğŸ’¯ unity vuforia android iphone
- [ ] furniture ar ğŸ”¢ unity easyar plane-detection backend-api
- [ ] volve show with ar ğŸ‘ğŸ½ unity vuforia 3d
- [ ]  zoomerang for swimming pool with arkit : unity arkit
- [ ] living texture - unity vuforia 3d

## facetracking
## arkit facetracking

![20220707_035840](https://user-images.githubusercontent.com/42961200/177623478-7fe32ac3-8ebb-448b-b717-2f4f18247cc1.gif)

This article shows some of the requirements for Face Tracking and features available in ARKit3:

https://9to5mac.com/2019/06/04/arkit-3-device-support/

"People Occlusion and the use of motion capture, simultaneous front and back camera, and multiple face tracking are supported on devices with A12/A12X Bionic chips, ANE, and TrueDepth Camera."

ARKit 3 goes further than ever before, naturally showing AR content in front of or behind people using People Occlusion, tracking up to three faces at a time, supporting collaborative sessions, and more. And now, you can take advantage of ARKitâ€™s new awareness of people to integrate human movement into your app.


## arcore facetracking
## arcore face tracking

https://user-images.githubusercontent.com/42961200/177655205-f12e949d-781e-49d7-b0fa-4a62e5236ded.mp4

![20220707_081647](https://user-images.githubusercontent.com/42961200/177658514-3c98060f-408b-42c0-8f5f-39da6fd2ab07.gif)


[Google](https://developers.google.com/ar/develop/augmented-faces)

The Augmented Faces API allows you to render assets on top of human faces without using specialized hardware. It provides feature points that enable your app to automatically identify different regions of a detected face. Your app can then use those regions to overlay assets in a way that properly matches the contours of an individual face.

![augmented-faces-468-point-face-mesh](https://user-images.githubusercontent.com/42961200/177655480-f50e49b4-6a26-441c-841d-0f4142b2b05d.png)

![image](https://user-images.githubusercontent.com/42961200/178147333-2ee6d93a-36bf-47c6-bb10-a94d626e3e12.png)

###  methods
ì¹´ë©”ë¼ì™€ ì¹´ë©”ë¼ ê·¸ë¦¬ê³  í”¼ì‚¬ì²´ì˜ ê³µê°„ íŒŒì•…
ì¹´ë©”ë¼ë¡œ ì¸ì‹ëœ íŠ¹ì • ì¸ë¬¼ ì¸ì‹
íŠ¹ì •ì¸ë¬¼ì˜ ì‹ ì²´ì  íŠ¹ì§• íŒŒì•…
ê°€ìƒê³µê°„ì˜ ë¦¬ê¹…ì´ ë˜ì–´ìˆëŠ” ìºë¦­í„°ì— ë§µí•‘
ì‹¤ì œ ì¹´ë©”ë¼ì˜ íŒ¬/í‹¸íŠ¸ ê°’ì˜ ì „ë‹¬
ê°€ìƒ ì¹´ë©”ë¼ì˜ íŒ¬/í‹¸íŠ¸ ê°’ ë°˜ì˜

### Methods
Hereâ€™s a simplified breakdown of the steps these artists follow:

1. Tracking the position, shape and movement of the face relative to the camera in 3D
2. Animation of the 3D models to snap on the tracked face (e.g. a dog nose)
3. Lighting and rendering of the 3D models into 2D images
4. Compositing of the rendered CGI images with the live action footage

4ì˜ ê²½ìš° 3D foregroundì— live backgroundë¡œ ê°€ëŠ¥. 2. 3ì€ ë¹„ë””ì˜¤ ê²Œì„ê³¼ ê°™ì´ ì²˜ë¦¬.

ìš”ëŠ” face trackingìœ¼ë¡œ êµ¬ê¸€ì´ë‚˜ ì• í”Œì˜ sdkë¥¼ í™œìš© í•˜ê±°ë‚˜, ML visionì„ í†µí•´ ì§ì ‘ ì œì‘.

### machine learning
object detection/tracking ë¨¸ì‰°ëŸ¬ë‹ì˜ ê¸°ë³¸ êµ¬ì¡°
![image](https://user-images.githubusercontent.com/42961200/178085012-f17ebee0-1289-4093-8a84-1a77c3ef117e.png)

í™œìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
MobileNet, SqueezeNet and ShuffleNet

ì´ì¤‘, ShuffleNet,
![image](https://user-images.githubusercontent.com/42961200/178085116-c088a049-92c6-4e17-aea1-7821d9f1ea1f.png)

Feature learning called landmarks
![image](https://user-images.githubusercontent.com/42961200/178085171-78443669-e234-4cdf-9695-20f9ba9754ce.png)

### dataset
>Building a training dataset

For augmenting videos with 3D effects, look for a dataset with 3D landmark coordinates such as 300W-LP.

![image](https://user-images.githubusercontent.com/42961200/178085258-d1e18803-e885-4e4a-b02e-492b8cfe10ed.png)

1. apply the following transformations to each image and landmark, to create new ones, by a random amount: rotation up to -/+ 40Â° around the centre, up to 10% translation and scale, and horizontal flip
2.  apply a different random transformation in memory on each image and for each learning pass (epoch) for additional augmentation.
3. build a Linux VM with a GeForce GTX 1080 Ti, prepare an OS and storage image where I setup PyTorch and upload my code and the datasets, all through ssh. Once the system is setup, it can be started and shut down on demand, creating a snapshot allows to resume the work 

### camera operation
ì¹´ë©”ë¼ êµ¬ë™ë¶€
Raspberry pi
Aduino
Python
ROS1

[![azurekinect_intelD455](https://img.youtube.com/vi/V2nimk44gQk/0.jpg)](https://youtu.be/watch?v=V2nimk44gQk)

{% include youtubePlayer.html id=page.youtubeId %}

![Image](https://user-images.githubusercontent.com/42961200/187614678-1657e611-0c05-4249-87da-e1abd2586dac.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614679-409707ac-685c-4af2-b745-114cf9a08b79.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614680-845798f8-b08b-4fd6-bd10-be011eaca857.jpg)



![Image](https://user-images.githubusercontent.com/42961200/187614823-2563ce02-a8e3-485b-a248-3b0bff939078.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614826-a51a36fb-300e-4e80-a35f-5bf5b626f16a.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614822-6c439a5a-27cf-4d4d-af36-566e71a354b2.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614824-9b61e242-8d8d-4b46-a20d-2b644b3ca457.jpg)
![Image](https://user-images.githubusercontent.com/42961200/187614825-58202f94-2170-4d0f-a7fd-b71cdcda87dd.jpg

### Singal processing

ì‹ í˜¸ ì²˜ë¦¬ë¶€
Ubuntu 20.04
Python 3.8
OpenCV
ROS1

### vision processing 
ì˜ìƒ ì²˜ë¦¬ë¶€
Windows 10
Unreal Engine C++
ROS1

{{site.data.alerts.bulletin}}
ì°¨ì„¸ëŒ€ ì„±ì¥ ë™ë ¥ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ë©”íƒ€ë²„ìŠ¤ì™€ ê´€ë ¨ëœ ë§ì€ ì•„ì´í…œ ì¤‘ì— Unityë‚˜ Unreal Engine ë“±ì˜ ê²Œì„ ì—”ì§„ì„ í™œìš©í•œ â€˜ì‹œë„¤ë§ˆí‹±â€™ì´ ìˆëŠ”ë°, ì´ ì‹œë„¤ë§ˆí‹±ìœ¼ë¡œ êµ¬í˜„ë˜ëŠ” ì˜í™”ë‚˜ ì—ë‹ˆë©”ì´ì…˜(ë§Œí™”)ì— ë“±ì¥í•˜ëŠ”  ì¼€ë¦­í„°(ì¸ë¬¼)ì˜ ì›€ì§ì„(ì—ë‹ˆë©”ì´ì…˜)ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ìŠˆíŠ¸ í˜•ì‹ì˜ ëª¨ì…˜ìº¡ì³ ì¥ë¹„ë¥¼ í™œìš©í•˜ê±°ë‚˜ ì˜¤í˜ëŸ¬ìŠ¤ ë“±ì˜ VRê¸°ê¸°ì˜ íŠ¸ë™ì»¤ ë“±ì„ í™œìš©í•˜ì—¬ ëª¨ì…˜ ìº¡ì³ì—
í™œìš©í•˜ëŠ” ê²ƒì´ ì•„ì§ê¹Œì§€ì˜ ê¸°ìˆ ì— í•œê³„ì˜€ìŠµë‹ˆë‹¤.

ì•„ì§ë„ VRê¸°ê¸°ì˜ ë¬´ê²ŒëŠ” ìƒë‹¹í•˜ê³  ê·¸ ì¡°ì‘ì„ ìœ„í•œ ì¥ë¹„ì˜ ê¸°êµ¬ì  í•œê³„ ë•Œë¬¸ì—ë””í…Œì¼í•œ í‘œí˜„ì´ ë˜ì§€ ì•Šê±°ë‚˜ ìˆ™ë ¨ëœ 3D ì‘ì—…ìë“¤ì´ ìƒë‹¹í•œ í›„ë°˜ ì‘ì—…ì„ í•´ì•¼ë§Œí•˜ëŠ” ì  ë•Œë¬¸ì— ì•„ì§ê¹Œì§€ë„ ì €íš¨ìœ¨ ê³ ë¹„ìš©ì˜ ìƒíƒœì…ë‹ˆë‹¤.
{{site.data.alerts.ends}}

{{site.data.alerts.note}}
ë³¸ í”„ë¡œì íŠ¸ëŠ” ê°ê° 2ëŒ€ì˜ ì¸¡ê±°ì ì´ ìˆëŠ” ì¹´ë©”ë¼ì™€ OpenCV ê¸°ìˆ ì„ í™œìš©í•˜ì—¬, í”¼ì‚¬ì²´ì™€ í•´ë‹¹ í”¼ì‚¬ì²´ê°€ ì¡´ì¬í•˜ëŠ” íŒŒì•…í•˜ì—¬
ê°€ìƒí˜„ì‹¤(å‡æƒ³ç¾å¯¦, ì˜ì–´: virtual reality, VR)ì˜ ê³µê°„ì—
í”¼ì‚¬ì²´ì˜ ì¸ì²´ì  íŠ¹ì§•ì„ ë°˜ì˜í•˜ì—¬ ìºë¦­í„°ë¥¼ ëª¸ì˜ ì›€ì§ì„(ì‹ ì²´ì˜ ì›€ì§ì„, ê³µê°„ì†ì— ì´ë™, í‘œì •, ì†ê°€ë½ì˜ ì›€ì§ì„)ì„ í†µí•´ ì¡°ì‘í•  ìˆ˜ ìˆëŠ” ê²ƒì´ íŠ¹ì§•ì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

ë˜í•œ ê° ì¹´ë©”ë¼ëŠ” í”¼ì‚¬ì²´ì˜ ì›€ì§ì„ì— ë”°ë¼ íŒ¬ê³¼ í‹¸íŠ¸ ì¡°ì‘ì´ ê°€ëŠ¥í•˜ë©°, ì´ ì¡°ì‘ì— ëŒ€í•œ ë³€í™”ê°’ë„ ROSë¥¼ í™œìš©í•˜ì—¬ ê°€ìƒê³µê°„ì— ì¡°ì‘ê°’ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
{{site.data.alerts.end}}

## ARcore detection
### machine learning

https://www.hackster.io/gr1m/raspberry-pi-facial-recognition-16e34e

![image](https://user-images.githubusercontent.com/42961200/177954400-2674b936-4414-4df4-81fa-c0cc57a15cf0.png)

https://aws.amazon.com/rekognition/

![image](https://user-images.githubusercontent.com/42961200/177954606-5401047a-cf88-4bdd-88e0-6ac6a0076859.png)

### Description
Pi-detector is used with [Pi-Timolo](https://github.com/pageauc/) to search motion generated images for face matches by leveraging AWS Rekognition. In its current state, matches are wrote to event.log. With some additional creativity and work, you could send out a notification or allow/deny access to a room with minimal changes. The install script will place the appropriate files in /etc/rc.loal to start on boot.

### Build Requirements
Raspberry Pi (Tested with Rpi 3)
Picamera
AWS Rekognition Access (Free tier options available)
As an alternative, this set of scripts can be modified to watch any directory that contains images. For example, if you collect still images from another camera and save them to disk, you can alter the image path to run facial recognition against any new photo that is created.

### AWS Rekognition
Before installing, it is best to get up and running on AWS. For my project, I am using the AWS Free Tier service. Using this allows you 5000 API calls per month, which is good enough for this project. Login to your console and create a new IAM user that has Administrative rights to Rekognition.

### Install
[Setup a Raspberry Pi with Raspbian Jessie](https://www.raspberrypi.org/downloads/raspbian)

SSH into your Raspberry pi (or connect it to a monitor and login using pi as the username and raspberry as the password). Don't forget to change the below IP address to your pi's IP. If you need help finding it on the network use nmap (nmap -sn 192.168.1.0/24)

```bash
ssh pi@192.168.1.120

```
Clone this repo and install:

```bash
git clone https://github.com/af001/pi-detector.git 
cd pi-detector/scripts 
sudo chmod +x install.sh 
sudo ./install.sh 

```
During the install, you will be prompted for your aws credentials that you set up earlier. When asked, enter your AWS Secret Key ID, AWS Secret Access Key, and set the region to us-east-1 (adjust to match the region you chose when you set up AWS Rekognition earlier). An example output would look something similar to the image below:

### Getting started
First, you need to create a new collection on AWS Rekognition. Creating a 'home' collection would look like:

```
cd pi-detector/scripts 
python add_collection.py -n 'home' 

```
Next, add your images to the pi-detector/faces folder. The more images of a person the better results you will get for detection. I would recommend several different poses in different lighting.

```
cd pi-detector/faces 
python ../scripts/add_image.py -i 'image.jpg' -c 'home' -l 'Tom' 

```
I found the best results by taking a photo in the same area that the camera will be placed, and by using the picam. If you want to do this, I created a small python script to take a photo with a 10 second delay and then puts it into the pi-detector/faces folder. To use it:

```
cd pi-detector/scripts 
python take_selfie.py 

```
Once complete, you can go back and rename the file and repeat the steps above to add your images to AWS Rekognition. Once you create a new collection, or add a new image, two reference files will be created as a future reference. These are useful if you plan on deleting images or collections in the future.

At this point, the setup is ready to go. You can setup Wi-Fi on your Rpi and place the camera where you want in your home. Once you plug in the Rpi, it should start working with no additional work needed from the user. To check your logs, just ssh into the Rpi and check the [event.log](http://event.log/) folder for a reference to your detections.

To delete a face from your collection, use the following:

```
cd pi-detector/scripts 
python del_faces.py -i '000-000-000-000' -c 'home' 

```
If you need to find the image id or a collection name, reference your faces.txt and collections.txt files.

To remove a collection:

```
cd pi-detector/scripts 
python del_collections.py -c 'home' 
```
Note that the above will also delete all the faces you have stored in AWS.

The last script is facematch.py. If you have images updated and just want to test static photos against the faces you have stored on AWS, do the following:

```
cd pi-detector/scripts 
python facematch.py -i 'tom.jpg' -c 'home' 
```
The results will be printed to screen, to include the percentage of similarity and confidence.

### 3D model

## 3D model
[github_eos](https://github.com/patrikhuber/eos)

[![4dface](https://img.youtube.com/vi/V3tPRI7vRZ0/0.jpg)](https://www.youtube.com/watch?v=V3tPRI7vRZ0)

{% include youtubePlayer.html id=page.youtubeId2 %}

![image](https://user-images.githubusercontent.com/42961200/178085685-f24884b1-e6a5-4f9a-a505-4a2f48e1fa43.png)

![image](https://user-images.githubusercontent.com/42961200/178085680-a5e69f2c-dd79-470b-8860-c6b0a6cb9d66.png)

To be able to build an app like Snapchat, we actually need more than a couple of 3D points. A common approach is to use a 3D Morphable Face Model (3DMM) and fit it on the annotated 3D points. The dataset 300W-LP actually comes with such data. The network could learn all the vertices of the 3D model instead of a small amount of landmarks, such that a 3D face will be predicted directly by the model and ready to use for some AR fun!

![image](https://user-images.githubusercontent.com/42961200/178085902-14e68cde-9273-428d-89f3-a9e8522b0e6c.png)

[![4dfm](https://img.youtube.com/vi/onutM3HzVoo/0.jpg)](https://youtu.be/watch?v=onutM3HzVoo)

### texture
![foxparts](https://user-images.githubusercontent.com/42961200/178374461-94532f6f-ecff-4068-bdd5-f32cc0a67e58.png)
The texture consists of eye shadow, freckles, and other coloring.
![FoxTexture](https://user-images.githubusercontent.com/42961200/178374490-90af84ed-8c87-418d-a286-47fe50bd9775.jpg)

{% include youtubePlayer.html id=page.youtubeId3 %}
{{site.data.alerts.hr_shaded}}
{% include youtubePlayer.html id=page.youtubeId4 %}


{% include taglogic.html %}

{% include links.html %}

{% include commento.html %}

{{site.data.alerts.hr_shaded}}
