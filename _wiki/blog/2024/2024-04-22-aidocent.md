---
layout: post
title: "Performance test for aidoncent based on EnglishTogether"
name: "aidocent"
tags: [ai nlp eng2]
tagName: nlp
permalink: 2024-04-22-aidocent.html
sidebar: other_sidebar
folder: blog
collection: wiki
categories: wiki
keywords: "ai aidocent nlp englishtogether uconcreative ucon"
summary: "Mon, Apr 22, 24, Aidocent setup to performance test within 5 days"
excerpt_separator: <!--more-->
toc: true
public: true
parent: [[Wiki-Setting-Category]] 
date: 2024-04-22T20:35:41 +0900
updated: 2024-04-22 20:35
---
* TOC
{:toc}

{{site.data.alerts.callout_warning}}This is a draft, the content is not complete and of poor quality!{{site.data.alerts.end}}

## Project post
[freelancer](https://www.freelancer.com/projects/python/Evaluation-Korean-Dataset-38019514/payments) Mohammad Sohaib
[linkedin](https://www.linkedin.com/in/thesohaib/) [upwork](https://www.upwork.com/freelancers/muhammadsohaib15?viewMode=1)

{{site.data.alerts.details}}
I need an experienced data scientist or AI specialist to assist with evaluating a Korean dataset using OpenAI API calls.
my goal is to set up a good testing template I can repeat

Key Aspects:
- Inference Speed: Achieving a balance of optimal inference speed is vital for this project. The desired inference speed for the evaluation stands at a moderate level when considering 2000 users.
- Total Processing Accuracy: I'm looking to ensure the total processing accuracy of this evaluation is high.
- TPA (Total Processing Accuracy): Aspects of TPA are crucial for this project; we aim for a balanced importance between total processing accuracy and inference speed. (<1 /s is imperative)
- Implement this on a shared googleColab or Jupyternotebook for on-demand use of the application

Ideal Skills and Experience:
- Proficiency in Korean language and understanding of language nuances.
- Previous experience with data evaluation using OpenAI API calls.
- Proven track record in achieving optimal inference speeds for data processing.
- Demonstrated ability to balance and optimize processing accuracy alongside inference speed.
- Strong communication skills to provide regular updates on the project's progress.

refer to this documentation;
https://python.langchain.com/docs/langsmith/walkthrough/

request for access to the example document where all the testing data and instructions should be maintained and updated;
https://docs.google.com/document/d/16GFgABbbnH7RXgL56SXNu41xRspstgtVcy0TQxmwd-A/edit?usp=drive_link

API example;
https://api-engtoprod.meta-wedit.com/api-docs#/USER%20API/UsersController_perpectConversation

deliverables;
The test procedure as described and per our discussion to the format of the sample google doc.
Set of scripts for JMeter configs, python for accuracy and inference test. (hopefully Colab)
A working PC implementation to run the test, on a machine and VM (dockerized) to replicate the same testing environment repeatedly.
A full documentation of test procedure, steps, screenshots, reference links, of running tests, and setups.

{{site.data.alerts.ended}}

### revised requirments
```
Key responsibilities:
- Conducting performance tests on LLMs.
- Analysing the results and comparing them to identify the best performing system.
- Providing recommendations on how to optimize the performance of the chosen LLM.
- Set up a local testing environment (chatbot, playground dashboard, Jmeter 5.5)
- Set up a cloud server (API server)
- Google Colab, Jupyter Notebook
- All configuration, test scripts in python, bash and exe)
- Dockerized images for future implementation
- Our Korean dataset of 20,000 published to HuggingFace workspace based on an existing Korean dataset
- documentation 1. testing items/procedures 2. instructions, manuals of this project.

Models to consider:
llama3-8b-8192
text-davinci-003
Mixtral 8x22b
Whisper

Reference:
https://pf7.eggs.or.kr/aigenerative_overview.html

Sample testing items (send me a permission request with your name plz)
https://docs.google.com/document/d/16GFgABbbnH7RXgL56SXNu41xRspstgtVcy0TQxmwd-A/edit

```

## Project resources
### googleDrive
#### overview
| [TestItems](https://docs.google.com/document/d/16GFgABbbnH7RXgL56SXNu41xRspstgtVcy0TQxmwd-A/edit?usp=sharing) | [uconc](https://docs.google.com/document/d/1Qlfhxnhlyy5p4i5WQqIelGMf1SRFGqq4Pz0XKmsQi9U/edit?usp=sharing)
| [q&aDataset](https://docs.google.com/spreadsheets/d/1UFqYntXEl5NkCUKuNfOowCKNDgDCWerB8kch5xf-xyc/edit#gid=0) /
[hfConverse](https://huggingface.co/datasets/jungsungmoon/Korean_dialog) | [hfQ&A](https://huggingface.co/datasets/nlpai-lab/kullm-v2) /
[hfQ&A2](https://huggingface.co/datasets/Ammad1Ali/Korean-conversational-dataset)
| [projectBlog](https://pf7.eggs.or.kr/2024-04-22-aidocent.html) | [hfQ&A3](https://huggingface.co/datasets/unoooo/alpaca-korean)

#### datasets
[hfDataset](https://huggingface.co/datasets) check under metrics for various testing items

HuggingFace Korean dataset 일상대화 : 다양한 질의답1  : 네이버 지식인 질의답 : 다양한 질의답2 : 

{% include image.html file="nlp/hfConverse.png" caption="koreanDataset" %}
{% include image.html file="nlp/hfQ&A.png" caption="koreanDataset1" %}
{% include image.html file="nlp/hfQ&A2.png" caption="koreanDataset2" %}
{% include image.html file="nlp/hfQ&A3.png" caption="koreanDataset3" %}

다양한 질의답2 : https://huggingface.co/datasets/unoooo/alpaca-korean

#### our dataset
[ucon-slmDatasets](https://huggingface.co/datasets/uconcreative/slmDataset)

#### other models
Models to consider:
llama3-8b-8192
text-davinci-003
Mixtral 8x22b
Whisper

### AWS server
[WindowsServer](13.239.112.86) port: on MS RemoteDesktop  [itsInstance](https://ap-southeast-2.console.aws.amazon.com/systems-manager/session-manager/i-0871c9912e7cb2174?region=ap-southeast-2#) accessible only via aws console

{% include image.html file="nlp/windowsServerAWS.png" caption="AWS Windows Server to run Jmeter to LLM application on the cloud" %}

[ApplicationServer]()

#### Windows Sever
> Setup, Jmeter

> Creds and IP

## mlOps
[googleColab]()

[JupyterNotebook]

### LLM candidates

| Models to consider: |llama3-8b-8192 | text-davinci-003 |Mixtral 8x22b |Whisper|

[misTral](https://docs.mistral.ai/) for mistral llm api docs

[gorqCloud](https://console.groq.com/docs/quickstart) and [playgroundConsole](https://console.groq.com/playground)

[ncSoftVaroq](https://github.com/kyopark2014/korean-chatbot-using-varco-llm-and-kendra)
 Korean LLM by NCSoft based varco llm and kendra

[gpt4all](https://github.com/aiegoo/gpt4all) locally hosted chatbot opensource.


Day 1, day2, day 3, day 4, day 5, day 6
{{site.data.alerts.details}}
Day1
1. Shared creds with freelancers
2. Task assigned among the group
3. AWS WindowServer instance
4. Completed the testing pc on the server
Day2
1. Application Server
2. API server
3. Lib install and setup
4. Download llm models
5. Write scripts
Day3 (today)
1. Write scripts using quantization of llms models
2. setup the application server.
3. pythong library issue
4. model candidates per quantization
5. flask app installed

{{site.data.alerts.ended}}

## performance test
[hfDatasetsMetrics](https://github.com/aiegoo/datasets/metrics)

### metrics
perplexity, accuracy, wer

[accuracy](https://github.com/aiegoo/datasets/tree/f96e74d5c633cd5435dd526adb4a74631eb05c43/metrics/accuracy)

[perplexity](https://github.com/aiegoo/datasets/tree/f96e74d5c633cd5435dd526adb4a74631eb05c43/metrics/perplexity)

[wer](https://github.com/aiegoo/datasets/tree/f96e74d5c633cd5435dd526adb4a74631eb05c43/metrics/wer)


{{site.data.alerts.detais}}

```
├───metrics
│   ├───accuracy
│   ├───bertscore
│   ├───bleu
│   ├───bleurt
│   ├───cer
│   ├───chrf
│   ├───code_eval
│   ├───comet
│   ├───competition_math
│   ├───coval
│   ├───cuad
│   ├───exact_match
│   ├───f1
│   ├───frugalscore
│   ├───glue
│   ├───google_bleu
│   ├───indic_glue
│   ├───mae
│   ├───mahalanobis
│   ├───matthews_correlation
│   ├───mauve
│   ├───mean_iou
│   ├───meteor
│   ├───mse
│   ├───pearsonr
│   ├───perplexity
│   ├───precision
│   ├───recall
│   ├───roc_auc
│   ├───rouge
│   ├───sacrebleu
│   ├───sari
│   ├───seqeval
│   ├───spearmanr
│   ├───squad
│   ├───squad_v2
│   ├───super_glue
│   ├───ter
│   ├───wer
│   ├───wiki_split
│   ├───xnli
│   └───xtreme_s
```

{{site.data.alerts.ended}}

{% include taglogic.html %}

{% include links.html %}

{% include commento.html %}

{{site.data.alerts.hr_shaded}}
